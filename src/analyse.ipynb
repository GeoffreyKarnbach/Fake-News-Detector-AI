{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.1 64-bit"
  },
  "interpreter": {
   "hash": "38740d3277777e2cd7c6c2cc9d8addf5118fdf3f82b1b39231fd12aeac8aee8b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Imports of pandas, seaborn"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from itertools import islice\n",
    "import nltk\n",
    "from nameparser.parser import HumanName\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "source": [
    "Download necessary ressources"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "source": [
    "Read train dataset from csv file"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/train.csv\")"
   ]
  },
  {
   "source": [
    "Statistical analysis about length of every \"text\" in dataset - For loop + Sorting by longest articles\n",
    "\n",
    "Generate graph of average length of articles in 100 of characters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_length = {}\n",
    "for content in df[\"text\"]:\n",
    "    try:\n",
    "        if len(content) in content_length:\n",
    "            content_length[len(content)]+=1\n",
    "        else:\n",
    "            content_length[len(content)] = 1\n",
    "    except: # Error in the dataset, one of the entries seems to be a float. To prevent execution stop: try/except.\n",
    "        pass\n",
    "\n",
    "content_length = dict(reversed(sorted(content_length.items(), key=lambda item: item[0]))) \n",
    "\n",
    "values = {}\n",
    "\n",
    "for loop in content_length.items():\n",
    "    if loop[0]//100 in values:\n",
    "        values[loop[0]//100]+=1\n",
    "    else:\n",
    "        values[loop[0]//100] = 1\n",
    "\n",
    "my_df = pd.DataFrame(values.items())\n",
    "ax = sns.barplot(x=0, y=1, data=my_df)\n",
    "ax.set(xlabel = '100 of characters', ylabel='Number of articles', title='Length of articles in 100 of characters')\n"
   ]
  },
  {
   "source": [
    "Statistical analysis about length of every fake news (label = 1) in dataset - For loop + Sorting by longest articles\n",
    "\n",
    "Generate graph of average length of articles in 100 of characters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_length = {}\n",
    "for content in df.loc[df['label'] == 1][\"text\"]:\n",
    "    try:\n",
    "        if len(content) in content_length:\n",
    "            content_length[len(content)]+=1\n",
    "        else:\n",
    "            content_length[len(content)] = 1\n",
    "    except: # Error in the dataset, one of the entries seems to be a float. To prevent execution stop: try/except.\n",
    "        pass\n",
    "\n",
    "content_length = dict(reversed(sorted(content_length.items(), key=lambda item: item[0]))) \n",
    "\n",
    "values = {}\n",
    "\n",
    "for loop in content_length.items():\n",
    "    if loop[0]//100 in values:\n",
    "        values[loop[0]//100]+=1\n",
    "    else:\n",
    "        values[loop[0]//100] = 1\n",
    "\n",
    "my_df = pd.DataFrame(values.items())\n",
    "ax = sns.barplot(x=0, y=1, data=my_df)\n",
    "ax.set(xlabel = '100 of characters', ylabel='Number of articles', title='Length of fake news in 100 of characters')\n"
   ]
  },
  {
   "source": [
    "Statistical analysis about length of every non fake news (label = 0) in dataset - For loop + Sorting by longest articles\n",
    "\n",
    "Generate graph of average length of articles in 100 of characters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_length = {}\n",
    "for content in df.loc[df['label'] == 0][\"text\"]:\n",
    "    try:\n",
    "        if len(content) in content_length:\n",
    "            content_length[len(content)]+=1\n",
    "        else:\n",
    "            content_length[len(content)] = 1\n",
    "    except: # Error in the dataset, one of the entries seems to be a float. To prevent execution stop: try/except.\n",
    "        pass\n",
    "\n",
    "content_length = dict(reversed(sorted(content_length.items(), key=lambda item: item[0]))) \n",
    "\n",
    "values = {}\n",
    "\n",
    "for loop in content_length.items():\n",
    "    if loop[0]//100 in values:\n",
    "        values[loop[0]//100]+=1\n",
    "    else:\n",
    "        values[loop[0]//100] = 1\n",
    "\n",
    "my_df = pd.DataFrame(values.items())\n",
    "ax = sns.barplot(x=0, y=1, data=my_df)\n",
    "ax.set(xlabel = '100 of characters', ylabel='Number of articles', title='Length of fake news in 100 of characters')"
   ]
  },
  {
   "source": [
    "Check amount of names and save result to CSV file."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_human_names(text):\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    pos = nltk.pos_tag(tokens)\n",
    "    sentt = nltk.ne_chunk(pos, binary = False)\n",
    "    person_list = []\n",
    "    person = []\n",
    "    name = \"\"\n",
    "    for subtree in sentt.subtrees(filter=lambda t: t.label() == 'PERSON'):\n",
    "        for leaf in subtree.leaves():\n",
    "            person.append(leaf[0])\n",
    "        if len(person) > 1: #avoid grabbing lone surnames\n",
    "            for part in person:\n",
    "                name += part + ' '\n",
    "            if name[:-1] not in person_list:\n",
    "                person_list.append(name[:-1])\n",
    "            name = ''\n",
    "        person = []\n",
    "\n",
    "    return (person_list)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "f = open(\"../data/name_output.csv\", 'w', encoding='UTF8', newline='')\n",
    "writer = csv.writer(f)\n",
    "writer.writerow([\"article_id\",\"nb_names\"])\n",
    "\n",
    "for id, content in enumerate(df[\"text\"]):\n",
    "    if id%100==0:\n",
    "        print(id)\n",
    "    try:\n",
    "        writer.writerow([id, len(get_human_names(content))])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(time.time()-start_time)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}