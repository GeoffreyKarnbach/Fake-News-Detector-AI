{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.1 64-bit"
  },
  "interpreter": {
   "hash": "38740d3277777e2cd7c6c2cc9d8addf5118fdf3f82b1b39231fd12aeac8aee8b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Imports of pandas, seaborn"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from itertools import islice\n",
    "import nltk\n",
    "from nameparser.parser import HumanName\n",
    "import time\n",
    "import csv\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(10,8)})\n",
    "sns.set_palette(\"hls\")"
   ]
  },
  {
   "source": [
    "Download necessary ressources"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "source": [
    "Read train dataset from csv file"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/kaggle/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "source": [
    "Statistical analysis about length of every \"text\" in dataset - For loop + Sorting by longest articles\n",
    "\n",
    "Generate graph of average length of articles in 100 of characters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"text_length\"] = df[\"text\"].apply(lambda x: math.ceil( len(str(x)) / 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(x=df[\"text_length\"])\n",
    "ax.locator_params(axis='x', nbins=20)\n",
    "ax.set(xlabel = 'Length of article(in 500s steps)', ylabel='Number of articles', title='Distribution of (all) article length')"
   ]
  },
  {
   "source": [
    "Statistical analysis about length of every fake news (label = 1) in dataset - For loop + Sorting by longest articles\n",
    "\n",
    "Generate graph of average length of articles in 100 of characters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(x=df.loc[df[\"label\"] == 1][\"text_length\"])\n",
    "ax.locator_params(axis='x', nbins=20)\n",
    "ax.set(xlabel = 'Length of article(in 500s steps)', ylabel='Number of articles', title='Distribution of (fake-news) article length')"
   ]
  },
  {
   "source": [
    "Statistical analysis about length of every non fake news (label = 0) in dataset - For loop + Sorting by longest articles\n",
    "\n",
    "Generate graph of average length of articles in 100 of characters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "ax = sns.countplot(x=df.loc[df[\"label\"] == 0][\"text_length\"])\n",
    "ax.locator_params(axis='x', nbins=20)\n",
    "ax.set(xlabel = 'Length of article(in 500s steps)', ylabel='Number of articles', title='Distribution (non-fake-news) of article length')"
   ]
  },
  {
   "source": [
    "Check amount of names and save result to CSV file."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "def get_human_names(text):\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    pos = nltk.pos_tag(tokens)\n",
    "    sentt = nltk.ne_chunk(pos, binary = False)\n",
    "    person_list = []\n",
    "    person = []\n",
    "    name = \"\"\n",
    "    for subtree in sentt.subtrees(filter=lambda t: t.label() == 'PERSON'):\n",
    "        for leaf in subtree.leaves():\n",
    "            person.append(leaf[0])\n",
    "        if len(person) > 1: #avoid grabbing lone surnames\n",
    "            for part in person:\n",
    "                name += part + ' '\n",
    "            if name[:-1] not in person_list:\n",
    "                person_list.append(name[:-1])\n",
    "            name = ''\n",
    "        person = []\n",
    "\n",
    "    return len(person_list)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "df[\"nb_names\"] = df[\"text\"].progress_apply(lambda x: get_human_names(str(x)))\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "source": [
    "Generate graph for fake news."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_names = pd.read_csv(\"../data/transformed/name_output.csv\")\n",
    "\n",
    "non_fake = {}\n",
    "fake = {}\n",
    "\n",
    "non_fake_sum = 0\n",
    "fake_sum = 0\n",
    "\n",
    "total_non_fake = 0\n",
    "total_fake = 0\n",
    "\n",
    "for index, row in df_names.iterrows():\n",
    "    \n",
    "    if df.loc[df[\"id\"] == row[\"article_id\"], \"label\"].to_string(index=False) == \"1\":\n",
    "\n",
    "        if row[\"nb_names\"] in fake:\n",
    "            fake[row[\"nb_names\"]]+=1\n",
    "        else:\n",
    "            fake[row[\"nb_names\"]] = 1\n",
    "        fake_sum += row[\"nb_names\"]\n",
    "        total_fake+=1\n",
    "\n",
    "    else:\n",
    "        if row[\"nb_names\"] in non_fake:\n",
    "            non_fake[row[\"nb_names\"]]+=1\n",
    "        else:\n",
    "            non_fake[row[\"nb_names\"]] = 1\n",
    "        \n",
    "        non_fake_sum += row[\"nb_names\"]\n",
    "        total_non_fake+=1\n",
    "\n",
    "print(f\"Non fake news amount: {total_non_fake}  -  Added up number of proper names: {non_fake_sum}  -  Average: {non_fake_sum/total_non_fake}\")\n",
    "print(f\"Fake news amount: {total_fake}  -  Added up number of proper names: {fake_sum}  -  Average: {fake_sum/total_fake}\")\n",
    "print(f\"All news amount: {total_fake+total_non_fake}  -  Added up number of all proper names: {fake_sum+non_fake_sum}  -  Average: {(fake_sum+non_fake_sum)/(total_fake+total_non_fake)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "ax = sns.countplot(x=df_wrangled.loc[df_wrangled[\"label\"] == 1][\"nb_names\"])\n",
    "ax.locator_params(axis='x', nbins=20)\n",
    "ax.set(xlabel = 'Number of names in article', ylabel='Number of articles', title='Distribution of article name usage in fake news')"
   ]
  },
  {
   "source": [
    "Generate graph for non fake news."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "ax = sns.countplot(x=df_wrangled.loc[df_wrangled[\"label\"] == 0][\"nb_names\"])\n",
    "ax.locator_params(axis='x', nbins=20)\n",
    "ax.set(xlabel = 'Number of names in article', ylabel='Number of articles', title='Distribution of article name usage in non-fake news')"
   ]
  },
  {
   "source": [
    "Amount of exclamation marks in fake/ non fake news"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_exclamation_counter = 0\n",
    "non_fake_exclamation_counter = 0\n",
    "\n",
    "for index, row in df.loc[df['text'].str.contains(r'!') == True].iterrows():\n",
    "    if row[\"label\"] == 0:\n",
    "        non_fake_exclamation_counter+=1\n",
    "    else:\n",
    "        fake_exclamation_counter+=1\n",
    "\n",
    "print(f\"Fake exclamation counter : {fake_exclamation_counter}\")\n",
    "print(f\"Non Fake exclamation counter : {non_fake_exclamation_counter}\")"
   ]
  },
  {
   "source": [
    "Check word length in articles"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{1: 15768797, 2: 15180728, 3: 12782209, 4: 9849455, 5: 7529657, 6: 5725692, 7: 4271272, 8: 2899080, 9: 1895428, 10: 1130369, 11: 633088, 12: 354174, 13: 193437, 14: 101077, 15: 53451, 16: 32009, 17: 20771, 18: 14872, 19: 10727}\n"
     ]
    }
   ],
   "source": [
    "def longer_than_(texte, nb):\n",
    "    counter = 0\n",
    "    for loop in str(texte).split(\" \"):\n",
    "        if len(loop)>=nb:\n",
    "            counter+=1\n",
    "    return counter\n",
    "\n",
    "word_length = {}\n",
    "for number in range(1,20):\n",
    "    word_length[number]=sum(df[\"text\"].apply(lambda x: longer_than_(x, number)))\n",
    "\n",
    "print(word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}